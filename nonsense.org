#+TITLE: Nonsense word amusement classification
#+STARTUP: showall latexpreview entitiespretty inlineimages

* <2020-12-01 Tue>

** Classification methods
Since my inputs are small (3 or 4 chars), how can I setup a simple ML algorithm that just does a linear classifier? Do I need to extract some basic feature vectors first?

*** N-gram
*** Representation learning + linear classifier
*** LSTM (like textgenrnn)
- Emojifier example in deeplearning.ai course 5 does sentiment classification on sentences. Try converting to working on just words.
*** Character-level convnet [[zotero://select/items/1_IBSDWNYP][Zhang, X., Zhao, J. & LeCun, Y. (2016) Character-level Convolutional Networks for Text Classification]]


*** stuff from NLTK book (older methods)
http://www.nltk.org/book/ch06.html

** Misc
- [ ] Investigate utility of [[https://hackernoon.com/chars2vec-character-based-language-model-for-handling-real-world-texts-with-spelling-errors-and-a3e4053a147d][Chars2Vec]]

** Papers
- [[zotero://select/items/1_P3D984VK][Badjatiya, P. et al. (2017) Deep Learning for Hate Speech Detection in Tweets]]
  - code in ~extra_code/twitter-hatespeech~. Good functional python scripting style!

** Keras RNN model
- use ~binary_crossentropy~ loss
- can /learn/ embeddings that are tuned towards the classification task

** Tensorflow tutorial on imbalanced class data [[https://www.tensorflow.org/tutorials/structured_data/imbalanced_data][link]]


* <2020-12-04 Fri>

** Karpathy's Recipe for Training neural networks blog post [[http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines][link]]
- Used note in here on initializing bias of final layer of network


** Papers
- [[zotero://select/items/1_38JECJP3][Kim, Y. et al. (2015) Character-Aware Neural Language Models]]
  - [ ] Look at this to see how a char-CNN can learn the relevant n-grams


* <2020-12-05 Sat>

** Misc
*** Text classification workflow and suggestions in this [[https://developers.google.com/machine-learning/guides/text-classification][Google guide]].
- [ ] try sepCNN. Code in section 4 of guide.

*** Tensorflow Text gen RNN [[https://www.tensorflow.org/tutorials/text/text_generation][tutorial]] puts embedding layer on CHARACTERS.
- [ ] does textgenrnn do this?
- [ ] try adding this to model.

*** Bayesian optimization in scikit learn: scikit-optimize
- ~BayesSearchCV~ does Bayes opt with interface similar to ~GridSearchCV~


** TODOs
- [X] Review F-score
- [X] Review AUC
- [ ] understand/implement character n-grams
- [ ] look at Vowpal Wabbit
- [ ] look up 'active learning' (is this the thing that tells you what examples to label?)
  - blog post [[https://medium.com/towards-artificial-intelligence/how-to-use-active-learning-to-iteratively-improve-your-machine-learning-models-1c6164bdab99][link]]
  - will use pool-based sampling
  - Active learning survery: [[zotero://select/items/1_7QUP3Z7Q][Settles, B. (2010) Active Learning Literature Survey]]
  - Active learning review blog post series: [[https://dsgissin.github.io/DiscriminativeActiveLearning/][link]]
  - 2007 paper on active learning for imbalanced classes: [[zotero://select/items/1_23FNCIEQ][Ertekin, S. et al. (2007) Learning on the border: active learning in imbalanced data classification]]
- [ ] look at using sklearn tree-based methods that use crossval methods that optimize on AUROC (which solves class imbalance problem)


* <2020-12-09 Wed>

** TODO Do simple classification on simplest 2 char inputs

** Input representation for MLP/Log reg
If each char is encoded via one-hot rep of dim 26, then an \( n \) character word has those \( n \) 26 dim reps concatentated. eg 4 char input to MLP is of size 26*4 = 104.


* <2021-10-08 Fri>

** TODO Fix Tensorflow errors on trying to run



